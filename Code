
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    classification_report, roc_curve
)

# 1. Load and Clean Data

df = pd.read_csv(r"C:/Users/Ummu Salma/Documents/smart_deal_recommendations.csv")
df = df.drop(columns=[col for col in df.columns if "Unnamed" in col])

# Define features and target
X = df.drop("redeemed", axis=1)
y = df["redeemed"]

categorical_cols = X.select_dtypes(include=["object"]).columns
numerical_cols = X.select_dtypes(include=["int64", "float64"]).columns


# 2. Explore Features

plt.figure(figsize=(6, 4))
sns.countplot(x=y)
plt.title("Coupon Redemption Distribution")
plt.show()


# 3. Preprocessing

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols)
    ]
)


# 4. Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# 5. Build Models

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.1, eval_metric="logloss",
                             random_state=42)
}

results = {}

for name, clf in models.items():
    pipe = Pipeline([
        ("preprocess", preprocessor),
        ("classifier", clf)
    ])

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    y_proba = pipe.predict_proba(X_test)[:, 1]

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1-score": f1_score(y_test, y_pred),
        "ROC-AUC": roc_auc_score(y_test, y_proba)
    }

    print("=====", name, "=====")
    print(classification_report(y_test, y_pred))

# =====================
# 6. Compare Results
# =====================
results_df = pd.DataFrame(results).T
print(results_df)

# 7. ROC Curve Example (Random Forest)

pipe_rf = Pipeline([
    ("preprocess", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=200, random_state=42))
])
pipe_rf.fit(X_train, y_train)
y_proba_rf = pipe_rf.predict_proba(X_test)[:, 1]

fpr, tpr, _ = roc_curve(y_test, y_proba_rf)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label="Random Forest")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()


# 8. Recommendation Example

sample = X_test.iloc[:1]
predicted = pipe_rf.predict(sample)
probability = pipe_rf.predict_proba(sample)[0, 1]

print("\nSample User Context:")
print(sample)
print("Predicted Redemption:", "Yes" if predicted[0] == 1 else "No")
print("Redemption Likelihood:", round(probability * 100, 2), "%")

